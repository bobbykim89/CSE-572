## 1. When solving for reinforcement learning, which method requires a perfect model of the environment?
- Dynamic Programming

## 2. When comparing policies in the Markov Decision Process (MDP), you can execute the policies for an infinite time and compare the accumulated rewards. What is the problem when using this evaluation method?
- It can result in infinite rewards, which consequently cannot be compared to each other.

## 3. Your colleagues designed a Markov Decision Process for autopilot modes of commercial airplanes, then asked for your advice on calculating rewards. What are the most common approaches for calculating rewards for different autopilot policies? (select all that apply)
- Finite horizon analysis
- Discounting rewards
- Expected reward

## 4. Which scenario will require the use of reinforcement learning?
- Sending a space rover to explore the surface of planet Jupiter

## 5. When compared to the Markov Decision Process, which statement describes an additional challenge of reinforcement learning?
- Exploring the environment

## 6. What is a shortcoming of using the Monte Carlo method for solving reinforcement learning problems?
- It uses a greedy approach, which is an approximation of the optimal solution and not necessarily the optimal solution.

## 7. What is a benefit of the state-action value function (Q(s,a)) over the state value function (V(s))?
- The state-action value function can be used in a greedy manner for choosing the best action at a current state.

## 8. When solving for reinforcement learning, which method is more suitable for continuous tasks?
- Temporal Difference Learning

## 9. When solving for reinforcement learning, which problem is suitable to solve using dynamic programming?
- Robot navigation in a room

## 10. What is Markovian property in the Markov Decision Process?
- The next state does not depend on past states, only on the current state and action.